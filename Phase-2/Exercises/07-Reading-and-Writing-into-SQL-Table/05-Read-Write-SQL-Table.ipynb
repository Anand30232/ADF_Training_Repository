{"cells":[{"cell_type":"markdown","source":["## Create Azure SQL Database\n- Import data using\n  - mysql-data.txt\n  - sqlserver.txt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1a260cd-fc02-4ede-8cde-4c84d1e8b970"}}},{"cell_type":"markdown","source":["## Create the JDBC URL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6457eaa-1348-4bc0-90cf-9238c16ba61d"}}},{"cell_type":"code","source":["jdbcHostname = \"sqlserveratin.database.windows.net\"\njdbcDatabase = \"sqldbatin\"\njdbcPort = 1433\nusername = \"atingupta2005\"\npassword = \"Azure@123456\"\njdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2};user={3};password={4}\".format(jdbcHostname, jdbcPort, jdbcDatabase, username, password)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fca3c272-2829-4fcd-8ea1-d8bfdd755af7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# You can pass in a dictionary that contains the credentials and driver class similar to the preceding Scala example.\n\njdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\nconnectionProperties = {\n  \"user\" : username,\n  \"password\" : password,\n  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96c997fb-3eae-4853-84a7-273bfd413b6b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e0aa093-28b5-48a6-8fe8-66e5316e8dd8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Push down a query to the database engine"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8af81167-5287-44d6-aa13-f88ef638b6da"}}},{"cell_type":"code","source":["pushdown_query = \"(select * from employees) employees_alias\"\ndf = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7bda746c-e18f-460f-a510-af6e3d657514"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[100,"Steven","King","steven.king@sqltutorial.org","515.123.4567","1987-06-17",4,"24000.00",null,9],[101,"Neena","Kochhar","neena.kochhar@sqltutorial.org","515.123.4568","1989-09-21",5,"17000.00",100,9],[102,"Lex","De Haan","lex.de haan@sqltutorial.org","515.123.4569","1993-01-13",5,"17000.00",100,9],[103,"Alexander","Hunold","alexander.hunold@sqltutorial.org","590.423.4567","1990-01-03",9,"9000.00",102,6],[104,"Bruce","Ernst","bruce.ernst@sqltutorial.org","590.423.4568","1991-05-21",9,"6000.00",103,6],[105,"David","Austin","david.austin@sqltutorial.org","590.423.4569","1997-06-25",9,"4800.00",103,6],[106,"Valli","Pataballa","valli.pataballa@sqltutorial.org","590.423.4560","1998-02-05",9,"4800.00",103,6],[107,"Diana","Lorentz","diana.lorentz@sqltutorial.org","590.423.5567","1999-02-07",9,"4200.00",103,6],[108,"Nancy","Greenberg","nancy.greenberg@sqltutorial.org","515.124.4569","1994-08-17",7,"12000.00",101,10],[109,"Daniel","Faviet","daniel.faviet@sqltutorial.org","515.124.4169","1994-08-16",6,"9000.00",108,10],[110,"John","Chen","john.chen@sqltutorial.org","515.124.4269","1997-09-28",6,"8200.00",108,10],[111,"Ismael","Sciarra","ismael.sciarra@sqltutorial.org","515.124.4369","1997-09-30",6,"7700.00",108,10],[112,"Jose Manuel","Urman","jose manuel.urman@sqltutorial.org","515.124.4469","1998-03-07",6,"7800.00",108,10],[113,"Luis","Popp","luis.popp@sqltutorial.org","515.124.4567","1999-12-07",6,"6900.00",108,10],[114,"Den","Raphaely","den.raphaely@sqltutorial.org","515.127.4561","1994-12-07",14,"11000.00",100,3],[115,"Alexander","Khoo","alexander.khoo@sqltutorial.org","515.127.4562","1995-05-18",13,"3100.00",114,3],[116,"Shelli","Baida","shelli.baida@sqltutorial.org","515.127.4563","1997-12-24",13,"2900.00",114,3],[117,"Sigal","Tobias","sigal.tobias@sqltutorial.org","515.127.4564","1997-07-24",13,"2800.00",114,3],[118,"Guy","Himuro","guy.himuro@sqltutorial.org","515.127.4565","1998-11-15",13,"2600.00",114,3],[119,"Karen","Colmenares","karen.colmenares@sqltutorial.org","515.127.4566","1999-08-10",13,"2500.00",114,3],[120,"Matthew","Weiss","matthew.weiss@sqltutorial.org","650.123.1234","1996-07-18",19,"8000.00",100,5],[121,"Adam","Fripp","adam.fripp@sqltutorial.org","650.123.2234","1997-04-10",19,"8200.00",100,5],[122,"Payam","Kaufling","payam.kaufling@sqltutorial.org","650.123.3234","1995-05-01",19,"7900.00",100,5],[123,"Shanta","Vollman","shanta.vollman@sqltutorial.org","650.123.4234","1997-10-10",19,"6500.00",100,5],[126,"Irene","Mikkilineni","irene.mikkilineni@sqltutorial.org","650.124.1224","1998-09-28",18,"2700.00",120,5],[145,"John","Russell","john.russell@sqltutorial.org",null,"1996-10-01",15,"14000.00",100,8],[146,"Karen","Partners","karen.partners@sqltutorial.org",null,"1997-01-05",15,"13500.00",100,8],[176,"Jonathon","Taylor","jonathon.taylor@sqltutorial.org",null,"1998-03-24",16,"8600.00",100,8],[177,"Jack","Livingston","jack.livingston@sqltutorial.org",null,"1998-04-23",16,"8400.00",100,8],[178,"Kimberely","Grant","kimberely.grant@sqltutorial.org",null,"1999-05-24",16,"7000.00",100,8],[179,"Charles","Johnson","charles.johnson@sqltutorial.org",null,"2000-01-04",16,"6200.00",100,8],[192,"Sarah","Bell","sarah.bell@sqltutorial.org","650.501.1876","1996-02-04",17,"4000.00",123,5],[193,"Britney","Everett","britney.everett@sqltutorial.org","650.501.2876","1997-03-03",17,"3900.00",123,5],[200,"Jennifer","Whalen","jennifer.whalen@sqltutorial.org","515.123.4444","1987-09-17",3,"4400.00",101,1],[201,"Michael","Hartstein","michael.hartstein@sqltutorial.org","515.123.5555","1996-02-17",10,"13000.00",100,2],[202,"Pat","Fay","pat.fay@sqltutorial.org","603.123.6666","1997-08-17",11,"6000.00",201,2],[203,"Susan","Mavris","susan.mavris@sqltutorial.org","515.123.7777","1994-06-07",8,"6500.00",101,4],[204,"Hermann","Baer","hermann.baer@sqltutorial.org","515.123.8888","1994-06-07",12,"10000.00",101,7],[205,"Shelley","Higgins","shelley.higgins@sqltutorial.org","515.123.8080","1994-06-07",2,"12000.00",101,11],[206,"William","Gietz","william.gietz@sqltutorial.org","515.123.8181","1994-06-07",1,"8300.00",205,11]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"employee_id","type":"\"integer\"","metadata":"{}"},{"name":"first_name","type":"\"string\"","metadata":"{}"},{"name":"last_name","type":"\"string\"","metadata":"{}"},{"name":"email","type":"\"string\"","metadata":"{}"},{"name":"phone_number","type":"\"string\"","metadata":"{}"},{"name":"hire_date","type":"\"date\"","metadata":"{}"},{"name":"job_id","type":"\"integer\"","metadata":"{}"},{"name":"salary","type":"\"decimal(8,2)\"","metadata":"{}"},{"name":"manager_id","type":"\"integer\"","metadata":"{}"},{"name":"department_id","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>email</th><th>phone_number</th><th>hire_date</th><th>job_id</th><th>salary</th><th>manager_id</th><th>department_id</th></tr></thead><tbody><tr><td>100</td><td>Steven</td><td>King</td><td>steven.king@sqltutorial.org</td><td>515.123.4567</td><td>1987-06-17</td><td>4</td><td>24000.00</td><td>null</td><td>9</td></tr><tr><td>101</td><td>Neena</td><td>Kochhar</td><td>neena.kochhar@sqltutorial.org</td><td>515.123.4568</td><td>1989-09-21</td><td>5</td><td>17000.00</td><td>100</td><td>9</td></tr><tr><td>102</td><td>Lex</td><td>De Haan</td><td>lex.de haan@sqltutorial.org</td><td>515.123.4569</td><td>1993-01-13</td><td>5</td><td>17000.00</td><td>100</td><td>9</td></tr><tr><td>103</td><td>Alexander</td><td>Hunold</td><td>alexander.hunold@sqltutorial.org</td><td>590.423.4567</td><td>1990-01-03</td><td>9</td><td>9000.00</td><td>102</td><td>6</td></tr><tr><td>104</td><td>Bruce</td><td>Ernst</td><td>bruce.ernst@sqltutorial.org</td><td>590.423.4568</td><td>1991-05-21</td><td>9</td><td>6000.00</td><td>103</td><td>6</td></tr><tr><td>105</td><td>David</td><td>Austin</td><td>david.austin@sqltutorial.org</td><td>590.423.4569</td><td>1997-06-25</td><td>9</td><td>4800.00</td><td>103</td><td>6</td></tr><tr><td>106</td><td>Valli</td><td>Pataballa</td><td>valli.pataballa@sqltutorial.org</td><td>590.423.4560</td><td>1998-02-05</td><td>9</td><td>4800.00</td><td>103</td><td>6</td></tr><tr><td>107</td><td>Diana</td><td>Lorentz</td><td>diana.lorentz@sqltutorial.org</td><td>590.423.5567</td><td>1999-02-07</td><td>9</td><td>4200.00</td><td>103</td><td>6</td></tr><tr><td>108</td><td>Nancy</td><td>Greenberg</td><td>nancy.greenberg@sqltutorial.org</td><td>515.124.4569</td><td>1994-08-17</td><td>7</td><td>12000.00</td><td>101</td><td>10</td></tr><tr><td>109</td><td>Daniel</td><td>Faviet</td><td>daniel.faviet@sqltutorial.org</td><td>515.124.4169</td><td>1994-08-16</td><td>6</td><td>9000.00</td><td>108</td><td>10</td></tr><tr><td>110</td><td>John</td><td>Chen</td><td>john.chen@sqltutorial.org</td><td>515.124.4269</td><td>1997-09-28</td><td>6</td><td>8200.00</td><td>108</td><td>10</td></tr><tr><td>111</td><td>Ismael</td><td>Sciarra</td><td>ismael.sciarra@sqltutorial.org</td><td>515.124.4369</td><td>1997-09-30</td><td>6</td><td>7700.00</td><td>108</td><td>10</td></tr><tr><td>112</td><td>Jose Manuel</td><td>Urman</td><td>jose manuel.urman@sqltutorial.org</td><td>515.124.4469</td><td>1998-03-07</td><td>6</td><td>7800.00</td><td>108</td><td>10</td></tr><tr><td>113</td><td>Luis</td><td>Popp</td><td>luis.popp@sqltutorial.org</td><td>515.124.4567</td><td>1999-12-07</td><td>6</td><td>6900.00</td><td>108</td><td>10</td></tr><tr><td>114</td><td>Den</td><td>Raphaely</td><td>den.raphaely@sqltutorial.org</td><td>515.127.4561</td><td>1994-12-07</td><td>14</td><td>11000.00</td><td>100</td><td>3</td></tr><tr><td>115</td><td>Alexander</td><td>Khoo</td><td>alexander.khoo@sqltutorial.org</td><td>515.127.4562</td><td>1995-05-18</td><td>13</td><td>3100.00</td><td>114</td><td>3</td></tr><tr><td>116</td><td>Shelli</td><td>Baida</td><td>shelli.baida@sqltutorial.org</td><td>515.127.4563</td><td>1997-12-24</td><td>13</td><td>2900.00</td><td>114</td><td>3</td></tr><tr><td>117</td><td>Sigal</td><td>Tobias</td><td>sigal.tobias@sqltutorial.org</td><td>515.127.4564</td><td>1997-07-24</td><td>13</td><td>2800.00</td><td>114</td><td>3</td></tr><tr><td>118</td><td>Guy</td><td>Himuro</td><td>guy.himuro@sqltutorial.org</td><td>515.127.4565</td><td>1998-11-15</td><td>13</td><td>2600.00</td><td>114</td><td>3</td></tr><tr><td>119</td><td>Karen</td><td>Colmenares</td><td>karen.colmenares@sqltutorial.org</td><td>515.127.4566</td><td>1999-08-10</td><td>13</td><td>2500.00</td><td>114</td><td>3</td></tr><tr><td>120</td><td>Matthew</td><td>Weiss</td><td>matthew.weiss@sqltutorial.org</td><td>650.123.1234</td><td>1996-07-18</td><td>19</td><td>8000.00</td><td>100</td><td>5</td></tr><tr><td>121</td><td>Adam</td><td>Fripp</td><td>adam.fripp@sqltutorial.org</td><td>650.123.2234</td><td>1997-04-10</td><td>19</td><td>8200.00</td><td>100</td><td>5</td></tr><tr><td>122</td><td>Payam</td><td>Kaufling</td><td>payam.kaufling@sqltutorial.org</td><td>650.123.3234</td><td>1995-05-01</td><td>19</td><td>7900.00</td><td>100</td><td>5</td></tr><tr><td>123</td><td>Shanta</td><td>Vollman</td><td>shanta.vollman@sqltutorial.org</td><td>650.123.4234</td><td>1997-10-10</td><td>19</td><td>6500.00</td><td>100</td><td>5</td></tr><tr><td>126</td><td>Irene</td><td>Mikkilineni</td><td>irene.mikkilineni@sqltutorial.org</td><td>650.124.1224</td><td>1998-09-28</td><td>18</td><td>2700.00</td><td>120</td><td>5</td></tr><tr><td>145</td><td>John</td><td>Russell</td><td>john.russell@sqltutorial.org</td><td>null</td><td>1996-10-01</td><td>15</td><td>14000.00</td><td>100</td><td>8</td></tr><tr><td>146</td><td>Karen</td><td>Partners</td><td>karen.partners@sqltutorial.org</td><td>null</td><td>1997-01-05</td><td>15</td><td>13500.00</td><td>100</td><td>8</td></tr><tr><td>176</td><td>Jonathon</td><td>Taylor</td><td>jonathon.taylor@sqltutorial.org</td><td>null</td><td>1998-03-24</td><td>16</td><td>8600.00</td><td>100</td><td>8</td></tr><tr><td>177</td><td>Jack</td><td>Livingston</td><td>jack.livingston@sqltutorial.org</td><td>null</td><td>1998-04-23</td><td>16</td><td>8400.00</td><td>100</td><td>8</td></tr><tr><td>178</td><td>Kimberely</td><td>Grant</td><td>kimberely.grant@sqltutorial.org</td><td>null</td><td>1999-05-24</td><td>16</td><td>7000.00</td><td>100</td><td>8</td></tr><tr><td>179</td><td>Charles</td><td>Johnson</td><td>charles.johnson@sqltutorial.org</td><td>null</td><td>2000-01-04</td><td>16</td><td>6200.00</td><td>100</td><td>8</td></tr><tr><td>192</td><td>Sarah</td><td>Bell</td><td>sarah.bell@sqltutorial.org</td><td>650.501.1876</td><td>1996-02-04</td><td>17</td><td>4000.00</td><td>123</td><td>5</td></tr><tr><td>193</td><td>Britney</td><td>Everett</td><td>britney.everett@sqltutorial.org</td><td>650.501.2876</td><td>1997-03-03</td><td>17</td><td>3900.00</td><td>123</td><td>5</td></tr><tr><td>200</td><td>Jennifer</td><td>Whalen</td><td>jennifer.whalen@sqltutorial.org</td><td>515.123.4444</td><td>1987-09-17</td><td>3</td><td>4400.00</td><td>101</td><td>1</td></tr><tr><td>201</td><td>Michael</td><td>Hartstein</td><td>michael.hartstein@sqltutorial.org</td><td>515.123.5555</td><td>1996-02-17</td><td>10</td><td>13000.00</td><td>100</td><td>2</td></tr><tr><td>202</td><td>Pat</td><td>Fay</td><td>pat.fay@sqltutorial.org</td><td>603.123.6666</td><td>1997-08-17</td><td>11</td><td>6000.00</td><td>201</td><td>2</td></tr><tr><td>203</td><td>Susan</td><td>Mavris</td><td>susan.mavris@sqltutorial.org</td><td>515.123.7777</td><td>1994-06-07</td><td>8</td><td>6500.00</td><td>101</td><td>4</td></tr><tr><td>204</td><td>Hermann</td><td>Baer</td><td>hermann.baer@sqltutorial.org</td><td>515.123.8888</td><td>1994-06-07</td><td>12</td><td>10000.00</td><td>101</td><td>7</td></tr><tr><td>205</td><td>Shelley</td><td>Higgins</td><td>shelley.higgins@sqltutorial.org</td><td>515.123.8080</td><td>1994-06-07</td><td>2</td><td>12000.00</td><td>101</td><td>11</td></tr><tr><td>206</td><td>William</td><td>Gietz</td><td>william.gietz@sqltutorial.org</td><td>515.123.8181</td><td>1994-06-07</td><td>1</td><td>8300.00</td><td>205</td><td>11</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c7e25b7-42ae-45f1-b188-2b57f587d555"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- employee_id: integer (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- phone_number: string (nullable = true)\n |-- hire_date: date (nullable = true)\n |-- job_id: integer (nullable = true)\n |-- salary: decimal(8,2) (nullable = true)\n |-- manager_id: integer (nullable = true)\n |-- department_id: integer (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- employee_id: integer (nullable = true)\n-- first_name: string (nullable = true)\n-- last_name: string (nullable = true)\n-- email: string (nullable = true)\n-- phone_number: string (nullable = true)\n-- hire_date: date (nullable = true)\n-- job_id: integer (nullable = true)\n-- salary: decimal(8,2) (nullable = true)\n-- manager_id: integer (nullable = true)\n-- department_id: integer (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["display(df.select(\"department_id\", \"salary\").groupBy(\"department_id\").avg(\"salary\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"636d354b-fdae-4734-abec-2885a99e6274"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"4400.000000"],[6,"5760.000000"],[3,"4150.000000"],[5,"5885.714286"],[9,"19333.333333"],[4,"6500.000000"],[8,"9616.666667"],[7,"10000.000000"],[10,"8600.000000"],[11,"10150.000000"],[2,"9500.000000"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"department_id","type":"\"integer\"","metadata":"{}"},{"name":"avg(salary)","type":"\"decimal(12,6)\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department_id</th><th>avg(salary)</th></tr></thead><tbody><tr><td>1</td><td>4400.000000</td></tr><tr><td>6</td><td>5760.000000</td></tr><tr><td>3</td><td>4150.000000</td></tr><tr><td>5</td><td>5885.714286</td></tr><tr><td>9</td><td>19333.333333</td></tr><tr><td>4</td><td>6500.000000</td></tr><tr><td>8</td><td>9616.666667</td></tr><tr><td>7</td><td>10000.000000</td></tr><tr><td>10</td><td>8600.000000</td></tr><tr><td>11</td><td>10150.000000</td></tr><tr><td>2</td><td>9500.000000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Read from JDBC connections across multiple workers"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6be565c8-42ec-492e-813a-e73d3ba9c60f"}}},{"cell_type":"code","source":["df = spark.read.jdbc(url=jdbcUrl, table=\"employees\", column=\"department_id\", lowerBound=1, upperBound=15, numPartitions=100, properties=connectionProperties)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"daa5aeec-e541-4e0c-94d9-c402b1512846"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"employee_id","type":"\"integer\"","metadata":"{}"},{"name":"first_name","type":"\"string\"","metadata":"{}"},{"name":"last_name","type":"\"string\"","metadata":"{}"},{"name":"email","type":"\"string\"","metadata":"{}"},{"name":"phone_number","type":"\"string\"","metadata":"{}"},{"name":"hire_date","type":"\"date\"","metadata":"{}"},{"name":"job_id","type":"\"integer\"","metadata":"{}"},{"name":"salary","type":"\"decimal(8,2)\"","metadata":"{}"},{"name":"manager_id","type":"\"integer\"","metadata":"{}"},{"name":"department_id","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>email</th><th>phone_number</th><th>hire_date</th><th>job_id</th><th>salary</th><th>manager_id</th><th>department_id</th></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Write data to JDBC"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab10b1d9-8577-4b3a-b868-f2655366ad00"}}},{"cell_type":"code","source":["df.select(\"employee_id\",\"first_name\", \"last_name\").write.format(\"jdbc\") \\\n  .mode(\"overwrite\") \\\n  .option(\"url\", jdbcUrl) \\\n  .option(\"dbtable\", \"dbo.Employees2\") \\\n  .option(\"user\", username) \\\n  .option(\"password\", password) \\\n  .save()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a42d2e1-fe06-4ca8-a760-a3d4ed080dbe"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Append to SQL Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6dfef3a-19cd-4345-a9e3-be5b02f681fa"}}},{"cell_type":"code","source":["try:\n  df.write \\\n    .format(\"jdbc\") \\\n    .mode(\"append\") \\\n    .option(\"url\", jdbcUrl) \\\n    .option(\"dbtable\", \"dbo.employees\") \\\n    .option(\"user\", username) \\\n    .option(\"password\", password) \\\n    .save()\nexcept ValueError as error :\n    print(\"Connector write failed\", error)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbcd3f26-ebee-4800-9118-919e1498ff79"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4235353068587888&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\">   </span>df<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>     <span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;jdbc&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>     <span class=\"ansi-blue-fg\">.</span>mode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;append&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span>     <span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;url&#34;</span><span class=\"ansi-blue-fg\">,</span> jdbcUrl<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">save</span><span class=\"ansi-blue-fg\">(self, path, format, mode, partitionBy, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1132</span>             self<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span>format<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1133</span>         <span class=\"ansi-green-fg\">if</span> path <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1134</span><span class=\"ansi-red-fg\">             </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1135</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1136</span>             self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    108</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    109</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 110</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    111</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o1561.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 4 times, most recent failure: Lost task 0.3 in stage 43.0 (TID 149) (10.139.64.4 executor driver): java.sql.BatchUpdateException: Cannot insert explicit value for identity column in table &#39;employees&#39; when IDENTITY_INSERT is set to OFF.\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2085)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:692)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:856)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:854)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1025)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1025)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2498)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:148)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2766)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2713)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2707)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2707)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1256)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2974)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2915)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2903)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1029)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2458)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2441)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2479)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2498)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2523)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1025)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1023)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:854)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:71)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:94)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:196)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:240)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:236)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:192)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:165)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:164)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:1079)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:126)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:267)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1079)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:468)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:311)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.sql.BatchUpdateException: Cannot insert explicit value for identity column in table &#39;employees&#39; when IDENTITY_INSERT is set to OFF.\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2085)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:692)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:856)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:854)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1025)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1025)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2498)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:148)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 4 times, most recent failure: Lost task 0.3 in stage 43.0 (TID 149) (10.139.64.4 executor driver): java.sql.BatchUpdateException: Cannot insert explicit value for identity column in table &#39;employees&#39; when IDENTITY_INSERT is set to OFF.","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4235353068587888&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\">   </span>df<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>     <span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;jdbc&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>     <span class=\"ansi-blue-fg\">.</span>mode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;append&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span>     <span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;url&#34;</span><span class=\"ansi-blue-fg\">,</span> jdbcUrl<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">save</span><span class=\"ansi-blue-fg\">(self, path, format, mode, partitionBy, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1132</span>             self<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span>format<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1133</span>         <span class=\"ansi-green-fg\">if</span> path <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1134</span><span class=\"ansi-red-fg\">             </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1135</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1136</span>             self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    108</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    109</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 110</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    111</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o1561.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 4 times, most recent failure: Lost task 0.3 in stage 43.0 (TID 149) (10.139.64.4 executor driver): java.sql.BatchUpdateException: Cannot insert explicit value for identity column in table &#39;employees&#39; when IDENTITY_INSERT is set to OFF.\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2085)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:692)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:856)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:854)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1025)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1025)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2498)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:148)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2766)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2713)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2707)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2707)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1256)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2974)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2915)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2903)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1029)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2458)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2441)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2479)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2498)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2523)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1025)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1023)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:854)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:71)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:94)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:196)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:240)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:236)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:192)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:165)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:164)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:1079)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:126)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:267)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1079)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:468)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:311)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.sql.BatchUpdateException: Cannot insert explicit value for identity column in table &#39;employees&#39; when IDENTITY_INSERT is set to OFF.\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2085)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:692)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:856)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:854)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1025)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1025)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2498)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:148)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["display(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c58b91b-9034-478f-baf9-bea5f2421635"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"employee_id","type":"\"integer\"","metadata":"{}"},{"name":"first_name","type":"\"string\"","metadata":"{}"},{"name":"last_name","type":"\"string\"","metadata":"{}"},{"name":"email","type":"\"string\"","metadata":"{}"},{"name":"phone_number","type":"\"string\"","metadata":"{}"},{"name":"hire_date","type":"\"date\"","metadata":"{}"},{"name":"job_id","type":"\"integer\"","metadata":"{}"},{"name":"salary","type":"\"decimal(8,2)\"","metadata":"{}"},{"name":"manager_id","type":"\"integer\"","metadata":"{}"},{"name":"department_id","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>email</th><th>phone_number</th><th>hire_date</th><th>job_id</th><th>salary</th><th>manager_id</th><th>department_id</th></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c30849cb-5045-4636-9608-02b1606e854e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"05-Read-Write-SQL-Table","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4235353068587875}},"nbformat":4,"nbformat_minor":0}
