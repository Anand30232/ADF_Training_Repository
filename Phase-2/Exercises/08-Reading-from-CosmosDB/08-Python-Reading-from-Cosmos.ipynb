{"cells":[{"cell_type":"markdown","source":["## Prerequisites\n- Azure Databricks Runtime 8.0 with Spark 3.1.1\n- Install Cosmos DB Spark Connector, in your spark Cluster\n  - https://search.maven.org/artifact/com.azure.cosmos.spark/azure-cosmos-spark_3-1_2-12/4.1.0/jar"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa8fd51f-c477-4d93-880f-3f8817959f26"}}},{"cell_type":"markdown","source":["## Create databases and containers\n- First, set Cosmos DB account credentials, and the Cosmos DB Database name and container name."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d5cd0af-289c-4001-a332-6d23ed658b21"}}},{"cell_type":"code","source":["cosmosEndpoint = \"https://cosmosdbatin.documents.azure.com:443/\"\ncosmosMasterKey = \"uWscEe78JP8Kw1Q6TxohCWur10aeG8nOXgrajYiiuxR13QTYpiVoBZRgJVk9Lu6EyHP9tzkQQNwfNGYzfn9s1w==\"\ncosmosDatabaseName = \"sampleDB\"\ncosmosContainerName = \"sampleContainer\"\n\ncfg = {\n  \"spark.cosmos.accountEndpoint\" : cosmosEndpoint,\n  \"spark.cosmos.accountKey\" : cosmosMasterKey,\n  \"spark.cosmos.database\" : cosmosDatabaseName,\n  \"spark.cosmos.container\" : cosmosContainerName,\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17cd614f-d3b8-4879-8caf-9381b5d069b9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["- Next, use the new Catalog API to create a Cosmos DB Database and Container through Spark."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20f4e5fe-f938-482d-9649-3eb2a60966c4"}}},{"cell_type":"code","source":["# Configure Catalog Api to be used\nspark.conf.set(\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\")\nspark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint\", cosmosEndpoint)\nspark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountKey\", cosmosMasterKey)\n\n# create a cosmos database using catalog api\nspark.sql(\"CREATE DATABASE IF NOT EXISTS cosmosCatalog.{};\".format(cosmosDatabaseName))\n\n# create a cosmos container using catalog api\nspark.sql(\"CREATE TABLE IF NOT EXISTS cosmosCatalog.{}.{} using cosmos.oltp TBLPROPERTIES(partitionKeyPath = '/id', manualThroughput = '1100')\".format(cosmosDatabaseName, cosmosContainerName))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e0fdc53-e0f0-41ef-a17d-eca9e53b25b8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[2]: DataFrame[]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: DataFrame[]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Ingesting data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee862665-86c2-47f7-add9-09aefc242ee5"}}},{"cell_type":"markdown","source":["- Write a memory dataframe consisting of two items to Cosmos DB:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d6b8361-1a73-4219-a7b5-3f89cd89c2ed"}}},{"cell_type":"code","source":["spark.createDataFrame(((\"cat-alive\", \"Schrodinger cat\", 2, True), (\"cat-dead\", \"Schrodinger cat\", 2, False)))\\\n  .toDF(\"id\",\"name\",\"age\",\"isAlive\") \\\n   .write\\\n   .format(\"cosmos.oltp\")\\\n   .options(**cfg)\\\n   .mode(\"APPEND\")\\\n   .save()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"091fba41-5ac2-470e-84ae-4b6aebf25580"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Querying data\n- Using the same cosmos.oltp data source, we can query data and use filter to push down filters:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"056631ef-e05d-4be5-bebc-e00133114460"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\ndf = spark.read.format(\"cosmos.oltp\").options(**cfg)\\\n .option(\"spark.cosmos.read.inferSchema.enabled\", \"true\")\\\n .load()\n\ndf.filter(col(\"isAlive\") == True)\\\n .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3387f04-92f6-4940-a77d-38204609c292"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---------+---------------+---+-------+\n|       id|           name|age|isAlive|\n+---------+---------------+---+-------+\n|cat-alive|Schrodinger cat|  2|   true|\n+---------+---------------+---+-------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+---------------+---+-------+\n       id|           name|age|isAlive|\n+---------+---------------+---+-------+\ncat-alive|Schrodinger cat|  2|   true|\n+---------+---------------+---+-------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Schema inference\n- When querying data, the Spark Connector can infer the schema based on sampling existing items by setting spark.cosmos.read.inferSchema.enabled to true."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b17517ea-203f-45bb-b5e0-590bc74a6d37"}}},{"cell_type":"code","source":["df = spark.read.format(\"cosmos.oltp\").options(**cfg)\\\n .option(\"spark.cosmos.read.inferSchema.enabled\", \"true\")\\\n .load()\n \ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc5691a2-1412-4f81-8a28-5e263f23c988"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- id: string (nullable = false)\n |-- name: string (nullable = false)\n |-- age: integer (nullable = false)\n |-- isAlive: boolean (nullable = false)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- id: string (nullable = false)\n-- name: string (nullable = false)\n-- age: integer (nullable = false)\n-- isAlive: boolean (nullable = false)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["- Alternatively, can pass the custom schema to be used to read the data:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30204183-8404-457a-a3bc-a92868e949ae"}}},{"cell_type":"code","source":["from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, BooleanType\ncustomSchema = StructType([\n      StructField(\"id\", StringType()),\n      StructField(\"name\", StringType()),\n      StructField(\"type\", StringType()),\n      StructField(\"age\", IntegerType()),\n      StructField(\"isAlive\", BooleanType())\n    ])\n\ndf = spark.read.schema(customSchema).format(\"cosmos.oltp\").options(**cfg)\\\n .load()\n \ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"614d527b-ee34-4c7c-8353-16defb3d378f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- type: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- isAlive: boolean (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- id: string (nullable = true)\n-- name: string (nullable = true)\n-- type: string (nullable = true)\n-- age: integer (nullable = true)\n-- isAlive: boolean (nullable = true)\n\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"08-Python-Reading-from-Cosmos","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4107664063981512}},"nbformat":4,"nbformat_minor":0}
